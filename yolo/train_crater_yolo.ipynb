{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d69d910",
   "metadata": {},
   "source": [
    "# YOLO Crater Detection Training Notebook\n",
    "Complete end-to-end training pipeline in one notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84958c",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81bb88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = ['ultralytics', 'opencv-python', 'torch', 'torchvision']\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', pkg, '-q'])\n",
    "    \n",
    "print(\"✓ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4529d1",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03674b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PyTorch: 2.9.1+cpu\n",
      "✓ GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import mathaa\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "print(f\"✓ GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e39931",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b248819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Model: yolov8n\n",
      "  Epochs: 10\n",
      "  Batch: 8\n",
      "  Image Size: 640\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "CONFIG = {\n",
    "    'gt_csv': Path('../train-sb/train-gt.csv'),\n",
    "    'images_root': Path('../train/train'),\n",
    "    'dataset_out': Path('./dataset'),\n",
    "    'runs_dir': Path('./runs'),\n",
    "    'model': 'yolov8n',  # yolov8n, yolov8s, yolov8m, yolov8l\n",
    "    'epochs': 10,\n",
    "    'batch': 8,\n",
    "    'imgsz': 640,\n",
    "    'train_ratio': 0.85,\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Image dimensions\n",
    "IMG_WIDTH = 2592.0\n",
    "IMG_HEIGHT = 2048.0\n",
    "CLASS_ID = 0  # crater\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Model: {CONFIG['model']}\")\n",
    "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"  Batch: {CONFIG['batch']}\")\n",
    "print(f\"  Image Size: {CONFIG['imgsz']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14adcbc6",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6355924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ellipse_to_yolo_bbox(cx, cy, semi_major, semi_minor, angle_deg):\n",
    "#     \"\"\"Convert ellipse to YOLO bbox (normalized)\"\"\"\n",
    "#     angle = math.radians(angle_deg)\n",
    "#     dx = math.sqrt((semi_major * math.cos(angle)) ** 2 + (semi_minor * math.sin(angle)) ** 2)\n",
    "#     dy = math.sqrt((semi_major * math.sin(angle)) ** 2 + (semi_minor * math.cos(angle)) ** 2)\n",
    "    \n",
    "#     x1 = max(0.0, cx - dx)\n",
    "#     x2 = min(IMG_WIDTH, cx + dx)\n",
    "#     y1 = max(0.0, cy - dy)\n",
    "#     y2 = min(IMG_HEIGHT, cy + dy)\n",
    "    \n",
    "#     w = x2 - x1\n",
    "#     h = y2 - y1\n",
    "#     if w <= 1e-6 or h <= 1e-6:\n",
    "#         return None\n",
    "    \n",
    "#     cx_n = (x1 + x2) / (2.0 * IMG_WIDTH)\n",
    "#     cy_n = (y1 + y2) / (2.0 * IMG_HEIGHT)\n",
    "#     w_n = w / IMG_WIDTH\n",
    "#     h_n = h / IMG_HEIGHT\n",
    "    \n",
    "#     if not (0.0 <= cx_n <= 1.0 and 0.0 <= cy_n <= 1.0):\n",
    "#         return None\n",
    "#     if w_n <= 0.0 or h_n <= 0.0:\n",
    "#         return None\n",
    "    \n",
    "#     return (cx_n, cy_n, w_n, h_n)\n",
    "\n",
    "\n",
    "# def read_annotations(gt_csv):\n",
    "#     \"\"\"Read crater annotations from CSV\"\"\"\n",
    "#     annotations = {}\n",
    "#     with gt_csv.open() as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             img_id = row[\"inputImage\"]\n",
    "#             cx = float(row[\"ellipseCenterX(px)\"])\n",
    "#             cy = float(row[\"ellipseCenterY(px)\"])\n",
    "#             semi_major = float(row[\"ellipseSemimajor(px)\"])\n",
    "#             semi_minor = float(row[\"ellipseSemiminor(px)\"])\n",
    "#             angle = float(row[\"ellipseRotation(deg)\"])\n",
    "#             annotations.setdefault(img_id, []).append((cx, cy, semi_major, semi_minor, angle))\n",
    "#     return annotations\n",
    "\n",
    "\n",
    "# def prepare_dataset(config):\n",
    "#     \"\"\"Prepare YOLO dataset\"\"\"\n",
    "#     print(\"Preparing dataset...\")\n",
    "    \n",
    "#     # Read annotations\n",
    "#     annotations = read_annotations(config['gt_csv'])\n",
    "#     print(f\"  Loaded {len(annotations)} unique images\")\n",
    "    \n",
    "#     # Split train/val\n",
    "#     all_ids = list(annotations.keys())\n",
    "#     random.Random(config['seed']).shuffle(all_ids)\n",
    "#     split_idx = int(len(all_ids) * config['train_ratio'])\n",
    "#     train_ids = sorted(all_ids[:split_idx])\n",
    "#     val_ids = sorted(all_ids[split_idx:])\n",
    "    \n",
    "#     # Create directories\n",
    "#     config['dataset_out'].mkdir(exist_ok=True)\n",
    "    \n",
    "#     # Process each split\n",
    "#     for split_name, img_ids in [(\"train\", train_ids), (\"val\", val_ids)]:\n",
    "#         img_count = 0\n",
    "#         for img_id in img_ids:\n",
    "#             src_img = config['images_root'] / f\"{img_id}.png\"\n",
    "#             if not src_img.exists():\n",
    "#                 continue\n",
    "            \n",
    "#             # Copy image\n",
    "#             dst_img = config['dataset_out'] / \"images\" / split_name / f\"{img_id}.png\"\n",
    "#             dst_img.parent.mkdir(parents=True, exist_ok=True)\n",
    "#             shutil.copy2(src_img, dst_img)\n",
    "            \n",
    "#             # Create label file\n",
    "#             dst_lbl = config['dataset_out'] / \"labels\" / split_name / f\"{img_id}.txt\"\n",
    "#             dst_lbl.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "#             bboxes = []\n",
    "#             for cx, cy, semi_major, semi_minor, angle in annotations.get(img_id, []):\n",
    "#                 bbox = ellipse_to_yolo_bbox(cx, cy, semi_major, semi_minor, angle)\n",
    "#                 if bbox:\n",
    "#                     bboxes.append(bbox)\n",
    "            \n",
    "#             with dst_lbl.open('w') as f:\n",
    "#                 for cx, cy, w, h in bboxes:\n",
    "#                     f.write(f\"{CLASS_ID} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "            \n",
    "#             img_count += 1\n",
    "        \n",
    "#         print(f\"  {split_name.upper()}: {img_count} images\")\n",
    "    \n",
    "#     print(\"✓ Dataset prepared\")\n",
    "\n",
    "\n",
    "# # Prepare dataset\n",
    "# prepare_dataset(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04877f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "  Loaded 4150 unique images\n",
      "  Reduced dataset to: Train=1000, Val=50\n",
      "  TRAIN: 1000 images\n",
      "  VAL: 50 images\n",
      "✓ Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "def ellipse_to_yolo_bbox(cx, cy, semi_major, semi_minor, angle_deg):\n",
    "    \"\"\"Convert ellipse to YOLO bbox (normalized)\"\"\"\n",
    "    angle = math.radians(angle_deg)\n",
    "    dx = math.sqrt((semi_major * math.cos(angle)) ** 2 + (semi_minor * math.sin(angle)) ** 2)\n",
    "    dy = math.sqrt((semi_major * math.sin(angle)) ** 2 + (semi_minor * math.cos(angle)) ** 2)\n",
    "    \n",
    "    x1 = max(0.0, cx - dx)\n",
    "    x2 = min(IMG_WIDTH, cx + dx)\n",
    "    y1 = max(0.0, cy - dy)\n",
    "    y2 = min(IMG_HEIGHT, cy + dy)\n",
    "    \n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    if w <= 1e-6 or h <= 1e-6:\n",
    "        return None\n",
    "    \n",
    "    cx_n = (x1 + x2) / (2.0 * IMG_WIDTH)\n",
    "    cy_n = (y1 + y2) / (2.0 * IMG_HEIGHT)\n",
    "    w_n = w / IMG_WIDTH\n",
    "    h_n = h / IMG_HEIGHT\n",
    "    \n",
    "    if not (0.0 <= cx_n <= 1.0 and 0.0 <= cy_n <= 1.0):\n",
    "        return None\n",
    "    if w_n <= 0.0 or h_n <= 0.0:\n",
    "        return None\n",
    "    \n",
    "    return (cx_n, cy_n, w_n, h_n)\n",
    "\n",
    "\n",
    "def read_annotations(gt_csv):\n",
    "    \"\"\"Read crater annotations from CSV\"\"\"\n",
    "    annotations = {}\n",
    "    with gt_csv.open() as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            img_id = row[\"inputImage\"]\n",
    "            cx = float(row[\"ellipseCenterX(px)\"])\n",
    "            cy = float(row[\"ellipseCenterY(px)\"])\n",
    "            semi_major = float(row[\"ellipseSemimajor(px)\"])\n",
    "            semi_minor = float(row[\"ellipseSemiminor(px)\"])\n",
    "            angle = float(row[\"ellipseRotation(deg)\"])\n",
    "            annotations.setdefault(img_id, []).append((cx, cy, semi_major, semi_minor, angle))\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def prepare_dataset(config):\n",
    "    \"\"\"Prepare YOLO dataset\"\"\"\n",
    "    print(\"Preparing dataset...\")\n",
    "    \n",
    "    # Read annotations\n",
    "    annotations = read_annotations(config['gt_csv'])\n",
    "    print(f\"  Loaded {len(annotations)} unique images\")\n",
    "    \n",
    "    # Split train/val\n",
    "    all_ids = list(annotations.keys())\n",
    "    random.Random(config['seed']).shuffle(all_ids)\n",
    "    split_idx = int(len(all_ids) * config['train_ratio'])\n",
    "    train_ids = sorted(all_ids[:split_idx])\n",
    "    val_ids = sorted(all_ids[split_idx:])\n",
    "\n",
    "    # --- Reduce dataset size ---\n",
    "    train_ids = train_ids[:1000]\n",
    "    val_ids = val_ids[:50]\n",
    "    print(f\"  Reduced dataset to: Train={len(train_ids)}, Val={len(val_ids)}\")\n",
    "    # ---------------------------\n",
    "    \n",
    "    # Create directories\n",
    "    config['dataset_out'].mkdir(exist_ok=True)\n",
    "    \n",
    "    # Process each split\n",
    "    for split_name, img_ids in [(\"train\", train_ids), (\"val\", val_ids)]:\n",
    "        img_count = 0\n",
    "        for img_id in img_ids:\n",
    "            src_img = config['images_root'] / f\"{img_id}.png\"\n",
    "            if not src_img.exists():\n",
    "                continue\n",
    "            \n",
    "            # Copy image\n",
    "            dst_img = config['dataset_out'] / \"images\" / split_name / f\"{img_id}.png\"\n",
    "            dst_img.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src_img, dst_img)\n",
    "            \n",
    "            # Create label file\n",
    "            dst_lbl = config['dataset_out'] / \"labels\" / split_name / f\"{img_id}.txt\"\n",
    "            dst_lbl.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            bboxes = []\n",
    "            for cx, cy, semi_major, semi_minor, angle in annotations.get(img_id, []):\n",
    "                bbox = ellipse_to_yolo_bbox(cx, cy, semi_major, semi_minor, angle)\n",
    "                if bbox:\n",
    "                    bboxes.append(bbox)\n",
    "            \n",
    "            with dst_lbl.open('w') as f:\n",
    "                for cx, cy, w, h in bboxes:\n",
    "                    f.write(f\"{CLASS_ID} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "            \n",
    "            img_count += 1\n",
    "        \n",
    "        print(f\"  {split_name.upper()}: {img_count} images\")\n",
    "    \n",
    "    print(\"✓ Dataset prepared\")\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "prepare_dataset(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc21ac62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bhgbvh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbhgbvh\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'bhgbvh' is not defined"
     ]
    }
   ],
   "source": [
    "bhgbvh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95197637",
   "metadata": {},
   "source": [
    "## 5. Create YOLO Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25171e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created crater.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create crater.yaml\n",
    "yaml_content = f\"\"\"path: {CONFIG['dataset_out'].resolve()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: 1\n",
    "names:\n",
    "  0: crater\n",
    "\"\"\"\n",
    "\n",
    "yaml_path = Path('./crater.yaml')\n",
    "yaml_path.write_text(yaml_content)\n",
    "print(f\"✓ Created {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab5a2a",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b36266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading yolov8n...\n",
      "\n",
      "Starting training...\n",
      "Ultralytics 8.3.252  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i3-1115G4 @ 3.00GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=crater.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\datashare\\yolo\\runs\\train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 93.635.9 MB/s, size: 2369.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\datashare\\yolo\\dataset\\labels\\train\\altitude01\\longitude02... 20 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 20/20 64.6it/s 0.3s0.2s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\datashare\\yolo\\dataset\\labels\\train\\altitude01\\longitude02.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 113.815.3 MB/s, size: 2575.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\datashare\\yolo\\dataset\\labels\\val\\altitude01\\longitude02... 5 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5/5 64.6it/s 0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\datashare\\yolo\\dataset\\labels\\val\\altitude01\\longitude02.cache\n",
      "Plotting labels to D:\\datashare\\yolo\\runs\\train3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mD:\\datashare\\yolo\\runs\\train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G      2.074      3.638      1.624         95        640: 100% ━━━━━━━━━━━━ 3/3 6.2s/it 18.6s7.6s1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.2s/it 2.2s\n",
      "                   all          5         99     0.0213      0.323     0.0304     0.0141\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G      1.733      3.455      1.539         79        640: 100% ━━━━━━━━━━━━ 3/3 5.4s/it 16.3s7.2s2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          5         99     0.0253      0.384     0.0314     0.0224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G       1.62      3.201      1.379         67        640: 100% ━━━━━━━━━━━━ 3/3 4.6s/it 13.9s7.3s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.9s/it 1.9s\n",
      "                   all          5         99     0.0307      0.465     0.0601     0.0408\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G      1.674      3.022      1.374         91        640: 100% ━━━━━━━━━━━━ 3/3 4.5s/it 13.5s5.8s5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.3s/it 2.3s\n",
      "                   all          5         99     0.0373      0.566     0.0715     0.0374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G      1.592      2.688      1.256         49        640: 100% ━━━━━━━━━━━━ 3/3 3.8s/it 11.5s5.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          5         99     0.0407      0.616     0.0838     0.0444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G      1.546      2.449      1.238         82        640: 100% ━━━━━━━━━━━━ 3/3 3.8s/it 11.5s6.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          5         99     0.0413      0.626     0.0968     0.0477\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G      1.486      2.181      1.206         66        640: 100% ━━━━━━━━━━━━ 3/3 4.8s/it 14.5s7.6s2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          5         99     0.0413      0.626      0.107     0.0579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G      1.348      1.987      1.179         88        640: 100% ━━━━━━━━━━━━ 3/3 6.7s/it 20.0s8.4s4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.5s/it 2.5s\n",
      "                   all          5         99     0.0427      0.646      0.176      0.117\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G      1.456      1.883      1.173         95        640: 100% ━━━━━━━━━━━━ 3/3 10.8s/it 32.3s12.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          5         99     0.0433      0.657      0.271      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G      1.451      1.892      1.171         71        640: 100% ━━━━━━━━━━━━ 3/3 8.5s/it 25.4s<12.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          5         99      0.044      0.667      0.307      0.219\n",
      "\n",
      "10 epochs completed in 0.056 hours.\n",
      "Optimizer stripped from D:\\datashare\\yolo\\runs\\train3\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from D:\\datashare\\yolo\\runs\\train3\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating D:\\datashare\\yolo\\runs\\train3\\weights\\best.pt...\n",
      "Ultralytics 8.3.252  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i3-1115G4 @ 3.00GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.3s/it 2.3s\n",
      "                   all          5         99      0.044      0.667      0.307       0.22\n",
      "Speed: 8.1ms preprocess, 229.3ms inference, 0.0ms loss, 7.4ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\datashare\\yolo\\runs\\train3\u001b[0m\n",
      "✓ Training completed\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {CONFIG['model']}...\")\n",
    "model = YOLO(f\"{CONFIG['model']}.pt\")\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "results = model.train(\n",
    "    data='crater.yaml',\n",
    "    epochs=CONFIG['epochs'],\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    batch=CONFIG['batch'],\n",
    "    device=device,\n",
    "    project=str(CONFIG['runs_dir']),\n",
    "    patience=10,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6745912",
   "metadata": {},
   "source": [
    "## 7. Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08784b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights: runs\\train3\\weights\\best.pt\n",
      "\n",
      "Validating...\n",
      "Ultralytics 8.3.252  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i3-1115G4 @ 3.00GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 708.0296.1 MB/s, size: 2575.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\datashare\\yolo\\dataset\\labels\\val\\altitude01\\longitude02.cache... 5 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5/5  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.0s/it 1.0s\n",
      "                   all          5         99      0.044      0.667      0.307       0.22\n",
      "Speed: 4.8ms preprocess, 116.4ms inference, 0.0ms loss, 4.7ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\datashare\\yolo\\runs\\detect\\val\u001b[0m\n",
      "\n",
      "✓ Validation Results:\n",
      "  mAP50: 0.3066\n",
      "  mAP: 0.2196\n",
      "  Precision: 0.0440\n",
      "  Recall: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# Get best weights path\n",
    "runs_detect = CONFIG['runs_dir'] \n",
    "latest_run = sorted(runs_detect.glob('*'))[-1]\n",
    "best_weights = latest_run / 'weights' / 'best.pt'\n",
    "\n",
    "print(f\"Best weights: {best_weights}\")\n",
    "\n",
    "# Load best model\n",
    "best_model = YOLO(str(best_weights))\n",
    "\n",
    "# Validate\n",
    "print(\"\\nValidating...\")\n",
    "metrics = best_model.val(data='crater.yaml', imgsz=CONFIG['imgsz'], device=device, split='val')\n",
    "\n",
    "print(\"\\n✓ Validation Results:\")\n",
    "print(f\"  mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP: {metrics.box.map:.4f}\")\n",
    "print(f\"  Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall: {metrics.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61254d",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62418910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 3 images...\n",
      "\n",
      "0: 512x640 (no detections), 184.1ms\n",
      "1: 512x640 (no detections), 184.1ms\n",
      "2: 512x640 (no detections), 184.1ms\n",
      "Speed: 3.5ms preprocess, 184.1ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mD:\\datashare\\yolo\\predictions\\test\u001b[0m\n",
      "\n",
      "✓ Inference Results:\n",
      "  Total detections: 0\n",
      "  Avg detections per image: 0.0\n",
      "  Results saved to: ./predictions/test\n"
     ]
    }
   ],
   "source": [
    "# Get test images\n",
    "test_dir = Path('../train/train/altitude08/longitude15')\n",
    "test_images = sorted(test_dir.glob('orientation*.png'))[:3]\n",
    "\n",
    "print(f\"Testing on {len(test_images)} images...\")\n",
    "\n",
    "# Run inference\n",
    "results = best_model.predict(\n",
    "    source=test_images,\n",
    "    conf=0.25,\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    device=device,\n",
    "    save=True,\n",
    "    project='./predictions',\n",
    "    name='test',\n",
    ")\n",
    "\n",
    "# Summary\n",
    "total_detections = sum(len(r.boxes) for r in results)\n",
    "print(f\"\\n✓ Inference Results:\")\n",
    "print(f\"  Total detections: {total_detections}\")\n",
    "print(f\"  Avg detections per image: {total_detections / len(test_images):.1f}\")\n",
    "print(f\"  Results saved to: ./predictions/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc533ee",
   "metadata": {},
   "source": [
    "## 9. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to ONNX\n",
    "# print(\"Exporting to ONNX...\")\n",
    "# exported_path = best_model.export(\n",
    "#     format='onnx',\n",
    "#     imgsz=CONFIG['imgsz'],\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# print(f\"✓ Model exported to: {exported_path}\")\n",
    "# print(f\"\\n✓ Training complete!\")\n",
    "# print(f\"  Best weights: {best_weights}\")\n",
    "# print(f\"  Exported: {exported_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d648d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6dc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc2bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing 26 boxes on orientation01_light01.png...\n",
      "Saved visualization to D:\\datashare\\yolo\\drawn_labels_check.png\n"
     ]
    }
   ],
   "source": [
    "# Function to draw Yolo bboxes\n",
    "def draw_yolo_labels(image_path, label_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    h_img, w_img = img.shape[:2]\n",
    "    \n",
    "    # Read labels\n",
    "    if not Path(label_path).exists():\n",
    "        print(f\"Error: Label file not found {label_path}\")\n",
    "        return\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    print(f\"Drawing {len(lines)} boxes on {Path(image_path).name}...\")\n",
    "        \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 5:\n",
    "            # YOLO format: class x_center y_center width height (normalized)\n",
    "            cls, cx_n, cy_n, w_n, h_n = map(float, parts[:5])\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            w_px = w_n * w_img\n",
    "            h_px = h_n * h_img\n",
    "            cx_px = cx_n * w_img\n",
    "            cy_px = cy_n * h_img\n",
    "            \n",
    "            x1 = int(cx_px - w_px / 2)\n",
    "            y1 = int(cy_px - h_px / 2)\n",
    "            x2 = int(cx_px + w_px / 2)\n",
    "            y2 = int(cy_px + h_px / 2)\n",
    "            \n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img, str(int(cls)), (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Save output to view it easily (or display it if using appropriate display tools)\n",
    "    out_path = Path('./drawn_labels_check.png')\n",
    "    cv2.imwrite(str(out_path), img)\n",
    "    print(f\"Saved visualization to {out_path.resolve()}\")\n",
    "\n",
    "# Define paths (Adjusting based on your request and likely relative paths in notebook context)\n",
    "# Assuming relative path from notebook root based on previous cells\n",
    "target_lbl = Path('dataset/labels/train/altitude01_longitude02_orientation01_light01.txt') # Note: previous cell flattened hierarchy in dataset/labels/train/\n",
    "target_img = Path('dataset/images/train/altitude01_longitude02_orientation01_light01.png')\n",
    "\n",
    "# If the user specifically provided absolute paths or a different structure in the query, we try to use those\n",
    "# The user prompt has specific paths with subfolders: altitude01/longitude02... \n",
    "# But the previous code (CELL 9) flattens the dataset structure: dataset/images/train/{img_id}.png\n",
    "# Let's try to construct the path based on the likely img_id from the user prompt filename.\n",
    "img_id = \"altitude01_longitude02_orientation01_light01\" # Usually constructed from flattened path\n",
    "# If the previous cells didn't flatten the structure, we would use grid.\n",
    "\n",
    "# Let's construct paths based on the CONFIG['dataset_out'] variable available in context\n",
    "# and the user's specific request structure.\n",
    "user_img_path = Path(r\"D:\\datashare\\yolo\\dataset\\images\\train\\altitude01\\longitude02\\orientation01_light01.png\")\n",
    "user_lbl_path = Path(r\"D:\\datashare\\yolo\\dataset\\labels\\train\\altitude01\\longitude02\\orientation01_light01.txt\")\n",
    "\n",
    "# Check if file exists, if not, try the flattened path standard in cell 9\n",
    "if not user_img_path.exists():\n",
    "    img_id_guess = \"altitude01_longitude02_orientation01_light01\"\n",
    "    user_img_path = CONFIG['dataset_out'] / 'images' / 'train' / f\"{img_id_guess}.png\"\n",
    "    user_lbl_path = CONFIG['dataset_out'] / 'labels' / 'train' / f\"{img_id_guess}.txt\"\n",
    "\n",
    "draw_yolo_labels(user_img_path, user_lbl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa470a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing 26 YOLO boxes on orientation01_light01.png...\n",
      "Drawing 26 ellipses for key 'altitude01/longitude02/orientation01_light01'...\n",
      "Saved visualization to D:\\datashare\\yolo\\drawn_labels_ellipses_check.png\n"
     ]
    }
   ],
   "source": [
    "# Function to draw YOLO bboxes + original ellipses + ellipse AABB\n",
    "def _infer_annotation_key_from_image_path(image_path):\n",
    "    p = Path(image_path)\n",
    "    parts = list(p.parts)\n",
    "    # Try nested dir form: .../altitudeXX/longitudeYY/orientationZZ_lightPP.png\n",
    "    for i, s in enumerate(parts):\n",
    "        if s.startswith('altitude') and i + 2 < len(parts):\n",
    "            alt = parts[i]\n",
    "            lon = parts[i + 1]\n",
    "            ori = parts[i + 2]\n",
    "            ori = Path(ori).stem  # remove .png if present\n",
    "            if lon.startswith('longitude') and ori.startswith('orientation'):\n",
    "                return f\"{alt}/{lon}/{ori}\"\n",
    "    # Fallback: flattened stem altitudeXX_longitudeYY_orientationZZ_lightPP\n",
    "    stem = Path(image_path).stem\n",
    "    toks = stem.split('_')\n",
    "    if len(toks) >= 4 and toks[0].startswith('altitude') and toks[1].startswith('longitude'):\n",
    "        return f\"{toks[0]}/{toks[1]}/{toks[2]}_{toks[3]}\"\n",
    "    return None\n",
    "\n",
    "def _normalize_img_id_for_annotations(img_id):\n",
    "    # Accept either \"altitude..../longitude..../orientation.._light..\" or flattened form\n",
    "    if img_id and '/' in img_id:\n",
    "        return img_id\n",
    "    if img_id:\n",
    "        toks = Path(img_id).stem.split('_')\n",
    "        if len(toks) >= 4:\n",
    "            return f\"{toks[0]}/{toks[1]}/{toks[2]}_{toks[3]}\"\n",
    "    return None\n",
    "\n",
    "def draw_yolo_labels_with_ellipses(image_path, label_path, annotations_dict=None, img_id=None, out_path=None):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image {image_path}\")\n",
    "        return\n",
    "\n",
    "    h_img, w_img = img.shape[:2]\n",
    "\n",
    "    # 1) Draw YOLO Bounding Boxes (Green)\n",
    "    if Path(label_path).exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"Drawing {len(lines)} YOLO boxes on {Path(image_path).name}...\")\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                cls, cx_n, cy_n, w_n, h_n = map(float, parts[:5])\n",
    "                w_px = w_n * w_img\n",
    "                h_px = h_n * h_img\n",
    "                cx_px = cx_n * w_img\n",
    "                cy_px = cy_n * h_img\n",
    "                x1 = int(cx_px - w_px / 2)\n",
    "                y1 = int(cy_px - h_px / 2)\n",
    "                x2 = int(cx_px + w_px / 2)\n",
    "                y2 = int(cy_px + h_px / 2)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, str(int(cls)), (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    else:\n",
    "        print(f\"Warning: Label file not found {label_path}\")\n",
    "\n",
    "    # 2) Draw original Ellipses (Blue) and their axis-aligned bbox (Yellow)\n",
    "    ann_key = None\n",
    "    if annotations_dict is not None:\n",
    "        ann_key = _normalize_img_id_for_annotations(img_id) if img_id else None\n",
    "        if not ann_key:\n",
    "            ann_key = _infer_annotation_key_from_image_path(image_path)\n",
    "\n",
    "        if ann_key and ann_key in annotations_dict:\n",
    "            ellipses = annotations_dict[ann_key]\n",
    "            print(f\"Drawing {len(ellipses)} ellipses for key '{ann_key}'...\")\n",
    "            for cx, cy, semi_major, semi_minor, angle in ellipses:\n",
    "                # Ellipse (Blue)\n",
    "                center = (int(round(cx)), int(round(cy)))\n",
    "                axes = (int(round(semi_major)), int(round(semi_minor)))\n",
    "                cv2.ellipse(img, center, axes, angle, 0, 360, (255, 0, 0), 2)\n",
    "\n",
    "                # Ellipse axis-aligned bounding box (Yellow)\n",
    "                ang = math.radians(angle)\n",
    "                dx = math.sqrt((semi_major * math.cos(ang)) ** 2 + (semi_minor * math.sin(ang)) ** 2)\n",
    "                dy = math.sqrt((semi_major * math.sin(ang)) ** 2 + (semi_minor * math.cos(ang)) ** 2)\n",
    "                x1 = int(max(0, min(w_img - 1, cx - dx)))\n",
    "                y1 = int(max(0, min(h_img - 1, cy - dy)))\n",
    "                x2 = int(max(0, min(w_img - 1, cx + dx)))\n",
    "                y2 = int(max(0, min(h_img - 1, cy + dy)))\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "        else:\n",
    "            print(f\"Warning: No annotations found for key '{ann_key}'\")\n",
    "    else:\n",
    "        print(\"Warning: annotations_dict is None; skipping ellipse drawing\")\n",
    "\n",
    "    # Save output\n",
    "    if out_path is None:\n",
    "        out_path = Path('./drawn_labels_ellipses_check.png')\n",
    "    cv2.imwrite(str(out_path), img)\n",
    "    print(f\"Saved visualization to {Path(out_path).resolve()}\")\n",
    "\n",
    "# --- Setup and run on one sample ---\n",
    "annotations_all = read_annotations(CONFIG['gt_csv'])\n",
    "\n",
    "# Try a known sample; auto-fallback to nested structure if needed\n",
    "img_id_guess = \"altitude01_longitude02_orientation01_light01\"\n",
    "test_img_path = CONFIG['dataset_out'] / 'images' / 'train' / f\"{img_id_guess}.png\"\n",
    "test_lbl_path = CONFIG['dataset_out'] / 'labels' / 'train' / f\"{img_id_guess}.txt\"\n",
    "\n",
    "# If flattened file doesn't exist, try nested folder structure\n",
    "if not test_img_path.exists():\n",
    "    nested_img = CONFIG['dataset_out'] / 'images' / 'train' / 'altitude01' / 'longitude02' / 'orientation01_light01.png'\n",
    "    nested_lbl = CONFIG['dataset_out'] / 'labels' / 'train' / 'altitude01' / 'longitude02' / 'orientation01_light01.txt'\n",
    "    if nested_img.exists():\n",
    "        test_img_path, test_lbl_path = nested_img, nested_lbl\n",
    "\n",
    "# Output path per image name\n",
    "out_vis = Path('./drawn_labels_ellipses_check.png')\n",
    "draw_yolo_labels_with_ellipses(test_img_path, test_lbl_path, annotations_all, img_id_guess, out_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3ef54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
